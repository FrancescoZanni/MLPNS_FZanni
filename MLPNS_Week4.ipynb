{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGKbY9XZgc77UixItoONZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoZanni/MLPNS_FZanni/blob/main/MLPNS_Week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 10\n",
        "##fine del GRB\n"
      ],
      "metadata": {
        "id": "XbnI7Ubq5TU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likelihood ratio test: \n",
        "Torno in ambito null hrt\n",
        "-2log(likelihood(modello complesso)/likelihood(modello semplice)\n",
        "dovrebbe essere distribuita con chi quadro, così posso vedere quale modello è piu semplice e scegliere in base al rasoio di Occam \n",
        "\n",
        "potrei implementare\n",
        "\n",
        "La frazione, che avrà sempre un valore maggiore di 1 perche più parametri da migliore likelihood, è abbastanza grande da giustificare l'aumento di parametri?\n",
        "\n",
        "è distr come uñchi sq distr, che ha come degrees of freedom la DIFFERENZA DEL NUMERO DI PARAMETRI \n",
        "\n",
        "una volta calcolato il valore vado nei valori critici della chi sq distr e controllo il pvalue confrontandolo al numero di sigma scelto in anticipo\n",
        "\n",
        "\n",
        "###Altri modelli: \n",
        "- AIC: si basa sulla stat frequentistica\n",
        "- BIC: si basa sul bayes theorem\n",
        "- MLD (minimum description lenght): si basa sul signal processing e information theory\n",
        "\n",
        "quasi tutti iguali con piccole differenze: tutti contengono una metrica delle performance del modello (likelihood) e una misura della complessità (numero di parametri)\n",
        "\n",
        "Li mettono insieme in modi diversi:\n",
        "- AIC ha un pezzo negativo, cioè che diminuisce per modelli miglpiori e un pezzo che cresce con la complessità del modelli, il goal è di ottenere un valore piccolo\n",
        "- BIC quasi uguale ma derivazione diversa, costanti differenti e la complessità del modello netra in due modi, anche il numero delle variabili.\n",
        "- MDL stesso principio, deriva dal bayes th, non importa tanto la formula, chissene della derivazione\n",
        "\n",
        "\n",
        "IMPLEMENTAZIONE (link nelle slide)\n",
        "\n",
        "###Come li utilizzo\n",
        "in generale cerco il valore per il quale sono minimizzati, ma non sempre sono minimizzati per lo stesso valore, potrei avere modelli preferiti diversi in base al test che ho scelto!\n",
        "\n",
        "L'utilizzo più appropriato è vedere quando la curva si stabilizza, cioè quando smette di scendere velocemente, e in quel caso dovrebbero dare lo stesso risultato. \n",
        "\n"
      ],
      "metadata": {
        "id": "MgF7O7Fm2C2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Unsipervised learning e clustering\n",
        "\n",
        "problema del clustering, si divide in 3 categorie\n",
        "- partitioning\n",
        "- density based clustering\n",
        "- hierarcical clustering\n",
        "\n",
        "se ho unsupervised non ho una variabile relazionata ai dati, \n",
        "voglio creare una nuova variabile che metto in relazione al featire space\n",
        "Per la maggior parte dei modelli diventa abbastanza ovvio, in neural nets sono tuttu supervised che si possono utilizzare per i goal dell'unsipervised learning: \n",
        "Voglio riprodurre una variabile che conosco o voglio creare una nuova variabile\n",
        "\n",
        "Motivo per l'unsipervised: voglio capire se ci sono relazioni fra i dati, per esempio se ho dati generati dallo stesso fenomeno oppure generati da fenomeni differenti\n",
        "\n",
        "oppure se posso trovare una descrizione dei dati che è meno dettagliata ma ugualmente soddisfacente: dimensionality reduction\n",
        "\n",
        "posso vedere se ci sono dei dati che sono particolarmente isolati:\n",
        "unsupervised learning enables anomaly detection!\n",
        "\n",
        "##modelli\n",
        "- PCS principal component analysis\n",
        "- Apriori\n",
        "- Clustering: il principale di cui ci occupiamo\n",
        "\n",
        "\n",
        "criteri per il clustering:\n",
        "-Criterio interno: membri dello stesso gruppo devono essere simili (compattezza del cluster: area piccola)\n",
        "-Criterio esterno: membri di gruppi diversi devono essere diversi (distanza dei cluster grande)\n",
        "\n",
        "Cose a cui devo pensare: \n",
        "- fra tutte le variabili che ho quali usare e come prepararle\n",
        "- come definisco la distanza\n",
        "- quale algoritmo utilizzare (dipenderà dalla distanza che ho scelto, non tutti gli algoritmi per tutte le distanze)\n",
        "\n",
        "OSS: per certi algoritmi il numero di cluster è un hyperparameter!! devo fare una guess e vedere se ottimale ecc.\n",
        "\n",
        "ideal algorithm fearures: VEDI SLIDE\n",
        "\n",
        "##DISTANCE METRICS\n",
        "(tutte le loss functions che usiamo sono sempre distanze)\n",
        "Famiglio di Minkowski: funziona se le variabili sono tutte numeriche, sono le varie Li\n",
        "\n",
        "4 regole per le distanze, vedi slide ma sone le solite\n",
        "unexpected \"ci sono distanze negative in altri spazi\", maggiore di 0 vale solo per la famiglia di mink...\n",
        "\n",
        "La metrica di Mink si attiene a tutte e 4\n",
        "\n",
        "L1: Manhattan distance (perchè a grid di strade perpendicolare)\n",
        "L2: distanza aerea, metrica euclidea\n",
        "\n",
        "OSS: la scelta della metrica è un hyperparameter!\n",
        "\n",
        "Le cose si complicano se ho bisogno di definire distanze in presenza di varaibili categoriche (\"Distanza fra un falco e una tigre\")\n",
        "\n",
        "Copresenza o coassenza di variabili binarie categoriche, per es presenza o no della coda o del becco, faccio la tabella di verità e mi da la distanza, poi posso farlo con n variabili categoriche binarie, la distanza è ricavata dalla tabella e ci sono vari metodi: \n",
        "\n",
        "- simple matching coefficient\n",
        "- jaccard similarity\n",
        "\n",
        "ecc.\n",
        "\n",
        "Esempio importante: \n",
        "applicazione più diretta: la jaccard similaruty è utilizzata in comp vision per definire la performance di un neural net che identifica la presenza di oggetti, cerca overlap, vedi slide ok  (\"intersection over union\")\n",
        "\n",
        "In questo caso la loss function sarà una jaccard similarity\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X07QAmSl5Q5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 11\n",
        "\n",
        "cose della lezione prima: unsupervised learning importante la definizione di distanza, non c'è una distanza migliore, dipende, la più comunqe è L2\n",
        "\n",
        "In generale va bene se le variabili sono numeriche e continue e se è una definizioen appropriata. Più la potenza cresce più i punti lontani dal core della distribuzione hanno peso sul modello(discorso già visto) \n",
        "\n",
        "Che distanza scelgo? non ho una risposta assoluta, domain question dipende dal problema e dalle variabili\n",
        "\n",
        "\n",
        "Vedi esercizio sulle categorie di distanze github ma non svolto\n",
        "\n",
        "Campo biomedico: categorie comuni: presenza o assenza di matattie, tratti ecc, ma anche pressione e temperatura.\n",
        "Come faccio clustering con variabili miste? \n",
        "Problema significativo, devo escogitare una distanza appropriata\n",
        "\n",
        "Blog tecnici: \n",
        "- Medium\n",
        "- Towards data science (link su slack e slides)\n",
        "\n",
        "RICORDO: posso usare il clustering come dimentionality reduction, stesse info ma in meno risorse"
      ],
      "metadata": {
        "id": "QirNzfWEvg3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Immagini\n",
        "array di pixel values, datatype int8: numero positivo fra 0 e 255\n",
        "a colori combino i tre canali \n",
        "\n",
        "In questo caso l feature space è l; intensità di ciascun pixel, quindi NxNx1 features\n",
        "\n",
        "clustering serve a definire i contorni, più cluster metto più vedo meglio gli edge ecc\n",
        "\n",
        "Comunque perdo dettagli della foto, ma la feature importante (il contorno del cane) rimane storato in un altro modo, quindi non perdo informazioni\n",
        "\n",
        "###applicazione scientifica\n",
        "NewYork, videocamere dell'urban observatory, cerco di fare inferenza sulla sociologia, economia ecc\n",
        "\n",
        "In questo caso di notte misuro l'energia consumata sulla base delle luci che vedo accese. \n",
        "Problema, devo individuare le luci singole, prima di poterle contare, chiedo di trovare cluster di pixel che hano le stesse caratteristiche, cioè luminosità alta, già con solo 2 cluster ho una bella selezione\n",
        "Con più cluster potrei anche distinguere luci di tipo differente.\n",
        "\n",
        "## Whitening (cosa più importante del corso)\n",
        "Tutti i risultati di qualunque algoritmo (a parte random forest cioè tree methods CART classif and regression trees ?)\n",
        "\n",
        "Il risultato dipende criticamente dal pre processing dei dati , in particolare dallo scaling e dal whitening dei dati: \n",
        "- whitening, si riferrisce al white noise: stessa intensità a tutte le frequenze\n",
        "problma della correlazione, che c'è quasi sempre !!! \n",
        "- variabili direttametne correlate, per esempio PIL e popolazione, oppure popolazione di un area e la quantità di traffico (una è il driver dell'altra)\n",
        "- Dipendono entrambe da una terza variabila che è il driver per entrambe (morte per annegamento e consumo di gelati, dipendono entrabe dalla temperatura, che è il driver) la differenza è tra la causation e la correlation\n",
        "\n",
        "\n",
        "\n",
        "Non sempre la correlazione è lineare! nel caos di lineare ho l'indice di pearson\n",
        "\n",
        "matrice di covarianza, se le variabili sono scorrelate diventa matrice diagonale, la covarianza è misurata sulla off diagonal\n",
        "\n",
        "\n",
        "##cosa c'entra la covarianza col clustering significato di WHITENING\n",
        "\n",
        "Se le variabili sono correlate si dispongono su una linea ma vorrei che stessero in un cerchio, dewvo passare al cerchio, bisogna risolvere la matrice di covarianza e diagonalizzarla, cosa difficile!\n",
        "Whitenin significa propio questo, passo a dati non correlati\n",
        "\n",
        "Comunemente non lo farema ma faremo standardizazione>: rendo la distribuzione su ogni variabile uan standard normal: p[orto la media a 0 e la standard deviation a 1, non ho garanzia che la forma sia sferica ma ho garanzia che le dimensioni siano compatibili\n",
        "\n",
        "Perchè è importante quando si fa clustering? \n",
        "Guardo il dataframe \n",
        "\n",
        "Se decido di fare clustering con una distanza euclidea con dati rpesi da distribuzioni molto diverse ho che l'asse più spreddato inciderà molto di più ruispetto all'altro!\n",
        "\n",
        "Questo, nello spazio in cui la vabiabili hanno significati differenti è un side effect\n",
        "\n",
        "Se me ne frego e faccio clustering così com'è farà cluster solo sul'asse che viene pesato di più, se invece correggo ho una divisione più onesta su entrambi gli assi\n",
        "\n",
        "Check notebook \n",
        "\n",
        "quello che faccio, come detto prima è sottrarre la media e dividere per la deviazione standard, così stesso spread\n",
        "\n",
        "sottraggo la media perchè è più \"utile\" amtematicamente lavorare su numeri dello stesso regime \n",
        "\n",
        "sim implementa ez e devo decidere su che assi voglio fare lo scaling, per esempio se sto facendo tiume series analysis non posso fare scaling sul tempo (??) altrtimenti misuro quanto sono diverse le medie ...\n",
        "\n",
        "In tutti gli altri casi se non ho una sequenzialità dei dati faccio scaling usando preprocessing.scale(X, axis=0)\n",
        "X dataframe , axis =  0 vuol dire che lo faccio su tutte le colonne"
      ],
      "metadata": {
        "id": "pHHYBj1xzEIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means\n",
        "\n",
        "è hard, cioè non mi da incertezze associate all assegnamento a un determinato cluster\n",
        "\n",
        "Crea delle regioni nel featiure space dove i punti appartengono a un cluster\n",
        "\n",
        "###Algoritmo\n",
        "1- Scelgo n punti a caso nel featrue space (pari al numero di cluster che voglio, hyperparameter) è un problema\n",
        "2- Inizio un processo iterativo: per ogni punto calcolo la distanza da ogni punto da ogniuno dei centri e assegno il punto al centro che è più vicino \n",
        "Si chiama k-means perchè il cluster è caratterizzato dal centro che è calcolato come la media dei punti nel cluster\n",
        "3- Ho un cluster, sposto il centro nella media dei punti che vi ho assegnato -> primo problema: come faccio la media tra uno scoiattolo e una balena? funziona solo con variabili numeriche e in più continue, se no esco dagli interi facendo la media\n",
        "4- Non ho più la certezza che i punti appartengano al cluster, ripeto l'assegnamento \n",
        "5- è cambiato l'assegnamento, sposto il centro e così vua\n",
        "\n",
        "Vado avanti fino alla convergenza: quando il centro si sposta poco, nnon uso quando cabia l'assegnamento, perchè potrei avere un punto a metà che oscilla \n",
        "\n",
        "terzo problema, la soluzione non è deterministica, in base a punti inziali differenti potrei finire in un punto diverso! \n",
        "La soluzione dipende dalla initial guess\n",
        "\n",
        "La objective function viene chiamata inertia ed è la distanza aggregata all'interno del cluster\n",
        "\n",
        "ORDINE: numero di cluster, numero di dimensioni, numero di iterazioni, numero di datapoints  O(KdN), idewalmente vorrei log n, questo non è male, ne vedremo di peggio \n",
        "\n",
        "Pro e contro nelle slide"
      ],
      "metadata": {
        "id": "SR_tob1yDNm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 12\n",
        "\n",
        "hierarcical clustering (bottom up) \n",
        "\n",
        "Comincio con n punti e vedo quali sono i punti più vicini...\n",
        "Li raggruppo secondo questa cosa \n",
        "\n",
        "a un certo punto noto che la distanza tra due cluster è minore di quella dei punti isolati (?) quindi unisco i cluster\n",
        "Continuo fino a quando non ho un solo cluster\n",
        "Pseudocodice nelle slide \n",
        "Non esiste convergenza, continuo fino a quando non ho un solo cluster, oppure un passo prima ma ok \n",
        "VANTAGGI: deterministico \n",
        "CONTRO: computazionalmente molto intenso: ad ogni passaggio ricalcolo la distanza di tutte le coppie di punti e di punto-cluster $O(N^2d+N^3)$\n",
        "\n",
        "è un algoritmo GREEDY: una volta che faccio una scelta quella è finalizzata, se ci fosse un ottimizzazione migliore non la trovo se ho già fatto la scelta,...\n",
        "\n",
        "Devo introdurre un linkage: come introduco la distanza tra due cluster? \n",
        "- Single link distance: i due punti più vicini\n",
        "- Complete link distance: i due punti più lontani\n",
        "- Centroid link distance media delle distanze \n",
        "- World distance (default), si basa sulla intracluster distance\n",
        "\n",
        "Divisive clustering: ...\n",
        "\n",
        "Le scelte sono salvare in dandrograms: alberi, una volta che arrivo alla fine posso basarmi sull'albero delle scelte per vedere il numero \"migliore\" dei cluster, posso estrarre il numero di cluster a posteriori, senza dover far andare una nuova volta l'algoritmo\n"
      ],
      "metadata": {
        "id": "Z6gwJP8VevDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73cJuhOWp3BD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "xCbh4_CXjnZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}