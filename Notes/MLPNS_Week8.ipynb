{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcBi1hKu3zlQJj/a3ntnm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoZanni/MLPNS_FZanni/blob/main/Notes/MLPNS_Week8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 22\n",
        "#Convolutional Neural Networks\n",
        "Appropriati peŕlavorare con immagini: l'idea è di astrarre forme(creo le forme nel training) caratteristiche dai diversi oggetti e cercare la presenza di queste forme all'interno delle immagini. \n",
        "\n",
        "Le cerco col dot product, operazione di sliding window  \n",
        "\n",
        "Aggiungo cose al notebook NN! (tensorflow)\n",
        "\n",
        "Torno alla convoluzione: \n",
        "- creo delle feature maps che contengono delle forme (fatte da 0 e 1 perchè la mia immagine ha questo formato)\n",
        "-fare la convoluzione significa far correre per tutti i pixel dell'immagine e dare come output il dot product dei pixel sovrapposti, e metto il risultato nell'output, che chiamo \"Feature Map\"\n",
        "-Devo applicare una funzione di attivazione, che per le reti convoluzionali spesso è la RELU, la applico a ciascun valore della feature map (attenzione le feature maps sono quello che stavamo estraendo da deepdream, la convoluzione essenzialmente esalta ciò che sto cercando)\n",
        "\n",
        "-Tipicamente tra i convolutional layers interpongo dei \"MAX_POOL layers\", che fanno un'operazione di DENOISING, riduco il rumore nelle informazioni che vengono dai layer precedenti, è un'aggregazione di pixel: riduce la dimensione della feature map, per esempio per ogni set di 2 pixel prendo quello col valore più alto (da qui max, potrei fare anche l'average pooling). \n",
        "Non devo abusare dei Maxpool per non arrivarea a immagini troppo piccole ! \n",
        "\n",
        "Continuo nel notebook Convolutional... \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "14wJ8jGp4GF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5VaUOFr3-HH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 23\n",
        "Tutto sul notebook Convolutional...\n"
      ],
      "metadata": {
        "id": "t-jaXL3aSPT2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TQWNTt9SQ1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phisically informed Neural Networks\n",
        "Fino agli anni 50 fisica era \"theory driven\", il processo scientifico procedeva cominciando dalla teoria (fasificabilità ecc). \n",
        "\n",
        "Dagi anni 80 a oggi siamo in regime \"data driven\".\n",
        "\n",
        "Utilizziamo spesso tanti dati e inferenza, non è fondamentale capire la natura dei dati se in ogni caso possiamo fare predizioni, se mi iteressa solo predire qualcosa mi posso limitare a fare random forest ecc.\n",
        "\n",
        "\n",
        "##Spazio intermedio: un sacco di dati ma non abbastanza per fare una automazione completa\n",
        "\n",
        "Non ho sufficienti dati per fare training ma ho anche conoscenza teorica \n",
        "\n",
        "Come posso insegnare alla mia rete che al di la del training esiste anche una teoria alla quale si deve attenere? cioè esistono delle boundary conditions del sistema?\n",
        "\n",
        "##Esempio comune Partial_non Linear diff equations\n",
        "derivata della funzione rispetto al tempo ha forma chiusa ma è difficile da risoverla perchè dipende dalle sue derivate: esempio \"flux function\".\n",
        "\n",
        "Ci sono approssimazioni lineari ma in quasi tutti i casi non si può calcolare la sol esatta se non in situazioni particolari e le soluzioni numeriche sono computazionalemnte troppo costose. \n",
        "\n",
        "L'idea è di combinare la soluzione analitica con un neural network(papers di Raissi): \\\\\n",
        "\n",
        "##Burgers equation \n",
        "Equazione del secondo ordine, non lineare, alle derivate parziali: \n",
        "\n",
        "Le applicazioni sono shockwaves, turbolenza, weather, traffico, acoustic transmission etc. \\\\\n",
        "Il parametro n è sempre la viscosità.  \n",
        "\n",
        "u è la velocità del fluido in una posizione a un determinato istante. \n",
        "Setto le boundary conditions(slides) \n",
        "\n",
        "###Implementazione\n",
        "L'idea è quella di creare un nN che risolva l'equazione: i so solo i dati alle boundary conditions, è l'input che do al NN: so la velocità al tempo zero e a tempo generico ma in posizione +1 e -1.\n",
        "\n",
        "La soluzione completa ha l'aspetto delle slide, il colore mappa le velocità. \n",
        "\n",
        "Per dare un input do a caso dati di contorno.\n",
        "\n",
        "Sto chiedendo al NN di estrapolare, non di interpolare, questo è un problema!!\n",
        "\n",
        "Ho però delle conoscenze su quello che deve succedere nella zona interna: qualsiasi velocità che venga calcolata deve rispettare la burgers equation!\n",
        "\n",
        "Devo inserirlo nel NN, come do questo contraint alla rete? \n",
        "L'unica cosa che ho è la loss function, quella che dice alla rete che sta facendo bene oppure no: quello che faccio è inserire la funzione di birg. nella loss function, lo faccio insieme a dei dati che però sono calcolati solo nelle B.C.\n",
        "\n",
        "\n",
        "Diamo al NN target points alle B. C. e inside gli diamo la sol della PDE. \n",
        "\n",
        "La mia Loss func sarà L2 per i punti che conosco e la PDE nei punti che non conosco. \n",
        "\n",
        "Il notebook è su Github e si trovano cose interessanti sul gitHub del gruppo del paper. \n",
        "\n",
        "PASSO A COMMENTARE IL CODICE!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LdZmANktelmw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qw2I0b9beyLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}