{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR5c41rLCRMCMvkiOObaW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoZanni/MLPNS_FZanni/blob/main/Notes/MLPNS_Week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lezione 16\n",
        "\n",
        "##Festure extraction (o engineering)\n",
        "\n",
        "Parto dal DF della volta scorsa, abbiamo usato la raw representation, che abbiamo standardizzato e per ogni timestamp (anno nella time series) ho confrontato il valore e come esso deviava dalla media di quella time series, in pratica abbiamo comparato la forma delle time series\n",
        "\n",
        "essenzialmente abbiamo trattato ogni data punto come una feature del mio feature space. \n",
        "C'è un'alternativa, spesso più usata rispetto alla raw rep.:estrazione di rappresentazioni alternative che creano un nuovo set di feature che posso usare per provare a creare un nuovo modell0 (?)\n",
        "\n",
        "Tipicamente il mio dataset ha un problema di dimensionalità: se voglio avere una curva temporale che è ricca devo necessariamente colezionare molti dati nel tempo: avere tanti timestamp\n",
        "\n",
        "Questo impone un feature space high dimentional (il nostro aveva 61 frature, cioè 61 anni/timestamp)\n",
        "Potrei avere anche diverse scale rilevanti per altri fenomeni\n",
        "\n",
        "Dovrei avere time scales molto lunghe e dettagliate, centinaia o migliai di timestamps.\n",
        "\n",
        "In questo caso di feature space con così tante dimensioni ho problemi di scaling degli algoritmi, che scalano in modo almeno lineare col numero delle features!\n",
        "\n",
        "MOtivo per il quale posso decidere di non utilizzare la raw rep. ma di trasformarle per ridurre la dimensionalità.\n",
        "\n",
        "Per quanto riguarda le time series potrei anvere anche time series che non sono sincrone. Potrei voler misurare curve a livello diurno o settimanale ma non è detto che sia sincrona, dovrei riallineare le mie time series sulla base della posizione dei picchi e delle valli (richiede un preprocessing difficile computazionalmente) \n",
        "Oppure potre cambiare rappresentazione: fitto un seno e uso ampiezza e frequenza come le features del nuovo spazio, passo a 2 dimensioni, che riduce di molto!\n",
        "\n",
        "Altro problema: warped time series, una potrebbe avere durata o forma diversa ma rappresentare lo stesso fenomeno\n",
        "\n",
        "PROBLEMA: raw reps high dimentional e similarità possono essere difficili da catturare in questo spazio\n",
        "Passo a rappresentazioni astratte, per es. media dei dati prima di standardizzarli, varianza, central tendency, momenti addizionali (skewness ecc..) Posso lavorare in questa descrizione statistica come feature space al posto dei dati originali.\n",
        "\n",
        "\n",
        "Oppure modelli parametrici per esprimere i dati, per esempio fittare e usare i parametri del fit o la goodness of fit, quanto il fit era ragionevole.\n",
        "\n",
        "Questo prende il nome di feature extraction\n",
        "\n",
        "Lista delle desiderata sulle slide: proprietà che vorrei ottenere nel nuovo feature space\n",
        "\n",
        "\n",
        "##Supervised Learning, Classification\n",
        "La target variable non è continua ma è categorica\n",
        "\n",
        "Come sempre devo definire una distanza, che sarà la objective function, così come nel clustering, ma per un motivo diverso\n",
        "\n",
        "Ho bisogno di avere le labels\n",
        "\n",
        "Il numero di oggetti di cui ho bisogno per avere un sistema efficiente nella classificazione dipende dalla dimensione del feature space ma anche dalla difficoltà di classificazione. \n",
        "\n",
        "###Esempio \n",
        "Dati punti di vari colori voglio classificare in modo da predire la variabile colore aggiungendo un punto nuovo (in generale per un test set) \n",
        "\n",
        "##SVM(Support Vector Machine)\n",
        "Crea un iperpiano che separa la regione con i vari colori, ovviamente trova quello che separa meglio, non in modo ottimale\n",
        "\n",
        "##Tree Methods\n",
        "Splitta lungo un asse alla volta creando \n",
        "\n",
        "Non mi devo preoccupare del whitening e dello scaling, non c'è più un concetto di peso!\n",
        "\n",
        "Molto diffuso se non so bene che cosa fare \n",
        "Se non ho idea che ci sia un motivo ontologico per cui un modello è meglii\n",
        "\n",
        "##KNearestNeighbors\n",
        "\n",
        "Mi interesso del concetto di distanza tra i punti, per ogni punto del dataset misuro la distanza da tutti gli altri punti, se volgio predire il colore sceglo il colore sulla base del colore dei punti che sono più vicini\n",
        "il K sarebbe l'hyperparameter che dice quanti vicini considero, solo il più vicino oppure medio(o scelgo il più popolare) sui primi 2-3- ecc\n",
        "\n",
        "\n",
        "Funziona molto bene se devo aggiungere dei punti, perchè mi interessano solo le distanze dai più vicini, non devo ricalcolare le distanze che già sapevo. \n",
        "Processo dispendioso all'inizio: N^2 distanze all'inizio, ciascuna distanza richiede d operazioni(d dimensione del feature space) \n",
        "Ma dopo aggiungo solo Nd \n",
        "O(N^2d+nd) dove n sono i punti nuovi\n",
        "\n",
        "###Algoritmo\n",
        "1- Calcolo la distanza da tutti i punti già presente\n",
        "\n",
        "2- Seleziono i K più vicini\n",
        "\n",
        "3- Assegno la feature più comune tra i più vicini\n",
        "\n",
        "Pros: \n",
        "- Non è parametrico, non devo decidere come è fatto l'hyperplane ecc\n",
        "- Funziona molto bene con training set grandi, dopo il costo iniziale \n",
        "- Per N grandi l'errore commesso dal 1-nearestneighbor è meno del doppio rispetto al migliore classificatore bayesiano (?)\n",
        "(importante perchè non è scontato che si conosca un upper limit dell'errore associato al modello che sto utilizzando)\n",
        "\n",
        "Ovviamente funziona solo per N infinito ma mi da un senso dell'ODG\n",
        "\n",
        "Cons:\n",
        "- Funziona solo se definisco la distanza \n",
        "- Dipende dalla scelta della distanza\n",
        "- Non è detto che freatures simili richiedono target variable simili, la predizione basata sulla vicinanza assume che similarità nel feature space implichi similarità nel labels space \n",
        "- Funziona male se il training set è sparso\n",
        "- Funziona male con gli Outliers, non ho un concetto di \"troppo distante quindi mi fido poco\"\n",
        "\n",
        "##Lazy learning\n",
        "Metodi che permettono di non ricalcolare tutto da capo, come in questo caso le distanze\n",
        "\n",
        "Molto utilizzati per esempio come reccomendation engines(netflix) netflix usa lazy learning, mantiene le cose vecchie e aggiunge un data punto in base all'ultima cosa che ho guardato \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I0Uz9KRZtUI6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZkd2-yOtQuw"
      },
      "outputs": [],
      "source": []
    }
  ]
}